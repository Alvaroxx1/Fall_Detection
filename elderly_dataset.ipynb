{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
    "def yolo2bbox(bboxes):\n",
    "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
    "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(image, bboxes, labels):\n",
    "    # Need the image height and width to denormalize\n",
    "    # the bounding box coordinates\n",
    "    h, w, _ = image.shape\n",
    "    for box_num, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = yolo2bbox(box)\n",
    "        # Denormalize the coordinates.\n",
    "        xmin = int(x1*w)\n",
    "        ymin = int(y1*h)\n",
    "        xmax = int(x2*w)\n",
    "        ymax = int(y2*h)\n",
    "\n",
    "        thickness = max(2, int(w/275))\n",
    "                \n",
    "        cv2.rectangle(\n",
    "            image, \n",
    "            (xmin, ymin), (xmax, ymax),\n",
    "            color=(0, 0, 255),\n",
    "            thickness=thickness\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images with the bounding boxes.\n",
    "def plot(image_paths, label_paths, num_samples):\n",
    "    all_images = []\n",
    "    all_images.extend(glob.glob(image_paths+'/*.jpg'))\n",
    "    all_images.extend(glob.glob(image_paths+'/*.JPG'))\n",
    "    \n",
    "    all_images.sort()\n",
    "\n",
    "    num_images = len(all_images)\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        j = random.randint(0,num_images-1)\n",
    "        image_name = all_images[j]\n",
    "        image_name = '.'.join(image_name.split(os.path.sep)[-1].split('.')[:-1])\n",
    "        image = cv2.imread(all_images[j])\n",
    "        with open(os.path.join(label_paths, image_name+'.txt'), 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                label = label_line[0]\n",
    "                bbox_string = label_line[2:]\n",
    "                x_c, y_c, w, h = bbox_string.split(' ')\n",
    "                x_c = float(x_c)\n",
    "                y_c = float(y_c)\n",
    "                w = float(w)\n",
    "                h = float(h)\n",
    "                bboxes.append([x_c, y_c, w, h])\n",
    "                labels.append(label)\n",
    "        result_image = plot_box(image, bboxes, labels)\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(result_image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(wspace=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('datasets/elderly/train/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few training images.\n",
    "plot(\n",
    "    image_paths='datasets/elderly/train/images/', \n",
    "    label_paths='datasets/elderly/train/labels/',\n",
    "    num_samples=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fall_v8.yaml\n",
    "path: 'D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/'\n",
    "train: 'D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/train/'\n",
    "val: 'D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/valid/'\n",
    "test: 'D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/test/'\n",
    "# class names\n",
    "names: \n",
    "  0: 'fall'\n",
    "  1: 'notfallen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default value is 0 set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check is cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def is_cuda_available():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "print(is_cuda_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ultralytics.utils import metrics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA']='1' \n",
    "# print(os.environ['CUDA_LAUNCH_BLOCKING'])\n",
    "# print(os.environ['TORCH_USE_CUDA_DSA'])\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train(\n",
    "    data='fall_v8.yaml', \n",
    "    epochs=10, \n",
    "    imgsz=1280, \n",
    "    batch=8,#works with batch=8\n",
    "    save_period=1,\n",
    "    device=[0],\n",
    "    optimizer='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model with an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "# Create a YOLO model\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Run the YOLO model on the input\n",
    "results = model.predict(source=\"D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/valid/images/img_104.jpg\", show=True)\n",
    "\n",
    "# Print the results\n",
    "print(results)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model with a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Create a YOLO model\n",
    "model = YOLO('../final_fall/trained_models/best.pt')\n",
    "\n",
    "# Define a video source\n",
    "video_source = '../final_fall/VideoFolder/fall4.mp4'\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "# Check if the video capture object was created successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "# Loop through the video\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was captured successfully\n",
    "    if ret:\n",
    "        # Run the YOLO model on the frame\n",
    "        results = model.predict(frame, show=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(results)\n",
    "        # Break the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=best.pt source=D:/Python_Env/HumanFallDetection/final_fall/datasets/elderly/valid/images imgsz=1280 name=yolov8n_v8_50e_infer1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize images in a 2x2 grid.\n",
    "def visualize(result_dir, num_samples=4):\n",
    "    \"\"\"\n",
    "    Function accepts a list of images and plots\n",
    "    them in a 2x2 grid.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    image_names = glob.glob(os.path.join(result_dir, '*.jpg'))\n",
    "    random.shuffle(image_names)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image = plt.imread(image_name)\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        if i == num_samples-1:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('D:/Python_Env/HumanFallDetection/runs/detect/yolov8n_v8_50e_infer1280')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
